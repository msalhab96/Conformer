checkpoint: null
device: 'cuda'

tokenizer:
  tokenizer_file: null
  vocab_path: 'files/vocab.txt'

data:
  sampling_rate: 16000
  n_mels: 80
  hop_length: 160
  n_ftt: 400
  win_length: ${data.n_ftt}
  spec_aug:
  freq_mask:
    max_freq: 27
  time_mask:
    ps: 0.05
    number_of_masks: 10
  training_file: 'files/train.csv'
  testing_file: 'files/test.csv'
  files_sep: ';'
  csv_file_keys:
    duration: 'duration'
    path: 'path'
    text: 'text'

training:
  batch_size: 16
  epochs: 100
  checkpoints_dir: 'checkpoints'
  optim:
    beta1: 0.9
    beta2: 0.98
    eps: 1e-9
    weight_decay: 1e-6
    warmup_staps: 1e4
    model_dim: ${model.model_dim}
    scaler: 0.05
    step_size: 1

model:
  model_dim: 512
  p_dropout: 0.1
  enc:
    enc_dim: ${model.model_dim}
    in_channels: ${data.n_mels}
    kernel_size: 25 # subsampling layer
    out_channels: 256 # subsampling layer
    mhsa_params:
      enc_dim: ${model.enc.enc_dim}
      h: 8 # number of heads
      p_dropout: ${model.p_dropout}
      device: ${device}
    conv_mod_params:
      enc_dim: ${model.enc.enc_dim}
      scaling_factor: 2
      kernel_size: 31
      p_dropout: ${model.p_dropout}
    feed_forward_params:
      enc_dim: ${model.enc.enc_dim}
      scaling_factor: 2
      p_dropout: ${model.p_dropout}
      residual_scaler: 0.5
    num_blocks: 4
    p_dropout: ${model.p_dropout}
  dec:
    enc_dim: ${model.enc.enc_dim}

